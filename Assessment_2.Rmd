---
title: "Assessment 2"
author: "Tolga Akturk"
date: '2022-03-20'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)
library(randomForest)

```

```{r}

thanksgiving<-read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-20/thanksgiving_meals.csv")

```

### Part 1: formatting RMarkdown document  (2 marks)

### 1. Create an Rmarkdown document with webpage as output (same as in setup)

At the start of the output document include your name in italic font and 
your student id in bold font as level 2 heading 

Separate with a solid line 

Include the title “Assignment 2” as level 1 heading 

Separate with a solid line 

List all tasks in the assignment as headings of the third level and include your results (=output) below each task showing your R code. 

## *Tolga Akturk* **s3876862**

---

# Assignment 2

---

### Part 2: Data Wrangling and visualization (38 marks)

### 1. Display the first 10 rows of the dataset using `kable()` function (1 marks)

```{r}

#This code below uses kable function to highlight the first ten rows of the dataset as why I have listed 1:10. 

knitr::kable(head(thanksgiving[, 1:10]))

```

### 2. Using `skim()` display the summary of variables. 

### Think about the task to predict a family income based on their menu: what variables may be useful? Are all of them correct type? Write 2-3 sentences with your explanation. (2 marks)


```{r}

#There are a few variables which I believe may be useful to answering this question, these include; main_dish, main_prep, stuffing, cranberry & gravy. It would be interesting to see what people eat as the main and what they can afford. I also chose these variables as a starting point as they had the most completed. I have never heard of some of these meals but after some googling can see prices vary e.g. Ham/Pork more expensive than Turkey. They are all in character form and would require as_factor function to change it into a factor.


```

### Think about the task to predict a community type or US_region based on their menu: what variables may be useful? Are all of them correct type? (2 marks)

```{r}

thanksgiving%>%
  count(community_type)

#I can already assume rural vs urban would be different in what is eaten at Thanksgiving. People in more affluent suburbs and urban areas might have more opportunity to purchasing a variety of mains and could also afford it vs those in rural areas and may continue family traditions more and have less accesibility. Same applies to us_regions, I would assume there is differences in regions of the US which would naturally impact Thanksgiving e.g. urban cities with much more multiculturalism vs country US outback regions with mainly Anglo American's reside. 
#I would include community_type, us_region, age, gender, family income and then also main_dish, main_prep as a start to begin exploring.They are all in character form and would require as_factor function to change it into a factor.

```

### 3. Use `fct_reorder` and `parse_number` functions to create a factor variable `family_income` (2 mark)


```{r}

#Firstly I will run this code to re-order the variable family_income and also changes this variable into a factor.
thanksgiving<-thanksgiving%>%
  mutate(family_income=fct_reorder(family_income, parse_number(family_income)))

#After this running skim on the data you will see that family_income is now a factor and also highlights the top counts in the variable.
#skim(thanksgiving)

#Running a count on the variable also now shows you from $0 to $200,000 and onwards in order.
thanksgiving%>%
  count(family_income)

```

### 4. What is the number of people who celebrate? (1 mark)

```{r}
thanksgiving%>%
count(celebrate)

#According to the data 980 people celebrated Thanksgiving from the total of 1,058 surveys completed. That is approximately 93% of the participants.You can view this by running a quick count on the variable count.
```

### 5. What are categories and insights for each main dish served and the method it is prepared? (2 marks)

```{r}

thanksgiving%>%
  count(main_dish)

#The categories are below; Chicken, Ham/Pork, Roast Beef, Tofurkey, Turkey, Turducken, I don't know, other and NA.

#Running a count check on main_dish we can clearly see that Turkey is the most popular main dish of Thanksgiving from this dataset. You could also comment that approximately 81% of those surveyed eat Turkey as their main dish at Thanksgiving by calculating this by 859 that chose Turkey / 1,058 total surveyed.


thanksgiving%>%
  count(main_prep)

#The categories are below; Baked, Fried, Roasted, I don't know, other and NA.

#The data highlights that there were a combination of methods to prepare Thanksgiving main dishes however Baked & Roasted primarily contributed to 481 & 378 respectively or 45% & 36% with total of 81% of the main dishes. 
  
```

### 6. Create 3 different data viz showing insights for main dish served and the method. Provide your own legend and use themes.

### Write 2-3 sentences with your explanation of each insight. (4 marks)

```{r}

thanksgiving%>%
  filter(!is.na(main_dish))%>%
  ggplot(aes(main_dish, main_prep, colour = main_dish)) +
  geom_jitter() +
  labs(x = "Main Dish Types", y = "Preparation Type", title ="Viewing Thanksgiving Main Dish and Preparation Types",
subtitle = "Clearly Turkey is our main either Roasted or Baked...",
caption = "81% is Turkey & either Baked or Roasted...") +
  theme_classic() +
  coord_flip()
 
#Turkey is a Thanksgiving favourite. This data visualisation highlights to the viewers that clearly the most popular Thanksgiving meal is Turkey and this is split almost 50/50 by roasted or baked. You can see this as the x asis highlights the number of people and on the y axis the chosen main meals which colour is added to see the meal prep of those meals. The colour is added to easily see the main dishes individually in this visualisation. I have also used the filter function at the beginning to remove NA resposes and have a more clearer view of the data viz.

```

```{r}


thanksgiving%>%
  count(us_region)

#After running a count (above) on us_region we can see that the surveyed Thanksgiving respondents are from; South Atlantic 214 people, Middle Atlantic 159 people, East North Central 150 people & Pacific with 146 people as for the most part. The combination of these US regions equates to over 63% of the surveyed respondents. 

thanksgiving%>%
  filter(!is.na(main_dish))%>%
  ggplot(aes(main_dish, us_region, colour = us_region)) +
  labs(x = "Main Dish Types", y = "US Region", title ="Viewing Thanksgiving Main Dish by US Regions",
subtitle = "South Atlantic, Middle Atlantic, East North Central & Pacific have the most Turkey...") +
  theme_classic() +
  geom_jitter()

#You can then see this in the visualisation below where the main Thanksgiving dish is served highlighted by the us_region. Turkey as we know is the favourite coming from Middle Atlantic, South Atlantic, Pacific and East North Central. 
```

```{r}
thanksgiving%>%
  count(gender)

#I ran a count on gender and according to the data there was 481 males and 544 females with a 45% and 51% respectively. The below data visualisation has on the x asis main dish preparation types and the y axis the gender. We cannot see any major differences from the above numbers when exploring main dish preparation types. 

thanksgiving%>%
  filter(!is.na(gender))%>%
  ggplot(aes(main_prep, gender, colour = gender)) +
  labs(x = "Main Dish Preparation Types", y = "Gender", title ="Viewing Thanksgiving Main Dish Prep by Gender",
subtitle = "Almost a 50/50 split with Male & Female....") +
  theme_classic() +
  geom_jitter()

```

### 7. How many use cranberry sauce? How many use gravy? 2marks

```{r}
thanksgiving%>%
  count(cranberry)

thanksgiving%>%
  count(gravy)

#803 people surveyed used cranberry sauce and 892 people surveyed used gravy.

```

### 8 - 9 What is the distribution of those who celebrate across income ranges. Create a data viz. Write 2-3 sentences with your explanation of each insight. (4 marks)

```{r}
#First I had to change celebrate from a character into a factor in order to use it in any data visualisation 
thanksgiving<-thanksgiving%>%
  mutate(celebrate=as_factor(celebrate))

#I skimmed the data to confirm this change
#skim(thanksgiving)

#I then filtered only those who celebrated Thanksgiving into my new dataset called celebrate.
celebrate<-thanksgiving%>%
  filter(celebrate=="Yes")

#I wanted to see a quick count of how many in each range. I realised that it was not ordered.
celebrate%>%
  count(family_income)

#I used fct_reorder and parse_number to organise the income ranges neatly.
celebrate<-celebrate%>%
  mutate(family_income=fct_reorder(family_income, parse_number(family_income)))

#This code below will highlight the distribution of family income of those who celebrated Thanksgiving.
celebrate%>%
  filter(!is.na(family_income))%>%
  ggplot(aes(family_income, colour = family_income)) +
  labs(x = "Family Income Ranges", y = "How Many People", title ="Distribution on Family Income across those who celebrated Thanksgiving",
subtitle = "...") +
  geom_bar() + 
  coord_flip()

#This data viz highlights that a large portion of those who celebrated Thanksgiving were on a family income of over $25,000 and below $125,000. It resembles are typical bell curve except after $200,000 it seems to spike and create and outlier. I would assume this is because the $200,000 and above is a huge bracket vs the main chunk which is limited into smaller brackets. There could be many people that earn more than $200,000 and could be the reason why a large portion of the people celebrate Thanksgiving, they can afford to. 
```

### 10. Use the following code to create a new data set 2 mark
### Write 2-3 sentences with your explanation of what it does. (4 marks)

```{r}


new_dataset<-thanksgiving%>%
select(id, starts_with("side"),
         starts_with("pie"),
         starts_with("dessert")) %>%
  select(-side15, -pie13, -dessert12) %>%
  gather(type, value, -id) %>%
  filter(!is.na(value),
         !value %in% c("None", "Other (please specify)")) %>%
  mutate(type = str_remove(type, "\\d+"))

#I believe what has happened here is firstly select. The new dataset called new_dataset will select from our original dataset called thanksgiving only these observations that match the pattern; side, pie & desert.
#It then pipes the dataset and runs select again but this time removing these three variables side15, pie13 & desert12.
#It then pipes again and gathers columns into key-value pairs; type, value and id.
#Then it pipes this and uses filter to remove from value any "None" and "Other" observations

#Overall what has happened here is that the original dataset was too difficult to use any side, pie or desert observations due to too many invalid observations. They either had nothing completed or had "None" or had "Other" which was too difficult to view. Now the new dataset can be used to compare within the three variables side, pie & desert without any errors.

```

### 11 - 12 Intall package `widyr` and use `pairwise_cor()` function https://www.rdocumentation.org/packages/widyr/versions/0.1.3/topics/pairwise_cor 

### Write 2-3 sentences with your explanation of what it does. (2 marks)

Use this code for the new dataset

`
pairwise_cor(value, id, sort = TRUE)

`
Write 1 sentence with your explanation of what insights it shows. (2 marks)

```{r}

library(widyr)

new_dataset%>%
  pairwise_cor(value, id, sort = TRUE)

#This shows the correlation between two items and we can see that the first two rows have the highest explained variance between them as Cookies and Brownies. It is comparing the pie, sides and deserts into a correlation to see what two variables have the highest explained variance. t doesn't mean Cookies and Brownies have any direct relationship per se but rather there is more variances explained with these two (0.4109...)
```


### 13. Use `lm()` or randomForest() function to build a model that predict a family income based on data in the dataset. 8 marks

Compare 3 models using different set of input variables. Use different number of variables.

Explain your choice of variables (3 sentences) 

Write 2 sentences explaining which model is best.

```{r}

#Model 1

#I chose work_retail to family_income because it is a known fact that the retail industry are on minimum awards and wages, in turn have less family income. So it would be interesting to see the results both with randomForest & lm. 

model_1<-randomForest(
  family_income~ work_retail,
  data = thanksgiving, 
  na.action=na.omit)

print(model_1)
#Difficult to interpret and whether I have done this correctly however it states a estimate error rate of 82.47%. 

model_1a<-lm(family_income~work_retail, data = thanksgiving)

print(model_1a)
#Negative coeffecient suggesting it may have a negative relationship.

#Model 2

model_2<-randomForest(
  family_income~main_dish+main_prep, 
  data = thanksgiving,
  na.action=na.omit)

print(model_2)
#The randomForest classifcation model results in an estimate error rate of 81.41%. 

model_2a<-lm(family_income~main_dish+main_prep, data = thanksgiving)

print(model_2a)
#This time comparing two inputs to family income I can see the coefficients are all negative except main_prep other with 0.226 and main_prep I don't know with 0.489. 

#Model 3

model_3<-randomForest(
  family_income~community_type,
  data = thanksgiving,
  na.action=na.omit)

print(model_3)
#Again an error rate of 82.47% between family income and community type.

model_3a<-lm(family_income~community_type, data = thanksgiving)

print(model_3a)
#The types of commmunity are negative coffecicent suggesting there isn't a positive relationship but rather a negative. Very difficult to interpret what this means or whether I have coded it correctly.

#I chose family income as the output and compared it to various variables. Model 1 was about more so retail workers vs family income. Model 2 comparing main dish and preparation to family income and lastly community type to income. 
#My reasons are clearly there is some insights with main dishes and how it differs with the income so I wanted to see it in the randomForest model. I wasn't sure if it was correct so I ran lm as well. 
#With this data and results I think model2 has the advantage with less error but I am still not convinced. 
```


