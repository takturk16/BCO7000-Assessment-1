---
title: "Assessment 2"
author: "Tolga Akturk"
date: '2022-03-20'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(tidyverse)
library(randomForest)

```

```{r}

thanksgiving<-read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-20/thanksgiving_meals.csv")

```

### Part 1: formatting RMarkdown document  (2 marks)

### 1. Create an Rmarkdown document with webpage as output (same as in setup)

At the start of the output document include your name in italic font and 
your student id in bold font as level 2 heading 

Separate with a solid line 

Include the title “Assignment 2” as level 1 heading 

Separate with a solid line 

List all tasks in the assignment as headings of the third level and include your results (=output) below each task showing your R code. 

## *Tolga Akturk* **s3876862**

---

# Assignment 2

---

### Part 2: Data Wrangling and visualization (38 marks)

### 1. Display the first 10 rows of the dataset using `kable()` function (1 marks)

```{r}

#This code below uses kable function to highlight the first ten rows of the dataset as why I have listed 1:10. 

knitr::kable(head(thanksgiving[, 1:10]))

```

### 2. Using `skim()` display the summary of variables. 

### Think about the task to predict a family income based on their menu: what variables may be useful? Are all of them correct type? Write 2-3 sentences with your explanation. (2 marks)


```{r}

skim(thanksgiving)

#The menu and by extension the menu variables are in this case the inputs and predicting family income is the output of this model. There are a few variables which I believe may be useful to answering this question, these include; main_dish, main_prep, stuffing, cranberry & gravy. The reason why I suggest these are that these variables have very few data points missing compared to the others which have 800+ missing data points. The results would not be conclusive as the data is weak. They also are not in their correct form. They are listed as "characters" which could not give us correct information. I would need to use as_factor to use them correctly. 


```

### Think about the task to predict a community type or US_region based on their menu: what variables may be useful? Are all of them correct type? (2 marks)

```{r}

#In this question we could model using the menu variables once again as my input, these can include; main_dish, main_prep, stuffing, cranberry & gravy. I would also include age & gender as my other input variables to see if there is any interesting insights. My output would be community_type and us_region. They are all once again not correct as they are listed as characters and will be needed to changed to factors for any data visualisation to occur. 

```

### 3. Use `fct_reorder` and `parse_number` functions to create a factor variable `family_income` (2 mark)


```{r}

#Firstly I will run this code to re-order the variable family_income and also changes this variable into a factor.
thanksgiving<-thanksgiving%>%
  mutate(family_income=fct_reorder(family_income, parse_number(family_income)))

#After this running skim on the data you will see that family_income is now a factor and also highlights the top counts in the variable.
skim(thanksgiving)

#Running a count on the variable also now shows you from $0 to $200,000 and onwards in order.
thanksgiving%>%
  count(family_income)


```

### 4. What is the number of people who celebrate? (1 mark)

```{r}
thanksgiving%>%
count(celebrate)

#According to the data 980 people celebrated Thanksgiving from the total of 1,058 surveys completed. That is approximately 93% of the participants.You can view this by running a quick count on the variable count.
```

### 5. What are categories and insights for each main dish served and the method it is prepared? (2 marks)

```{r}

thanksgiving%>%
  count(main_dish)

#The categories are below; Chicken, Ham/Pork, Roast Beef, Tofurkey, Turkey, Turducken, I don't know, other and NA.

#Running a count check on main_dish we can clearly see that Turkey is the most popular main dish of Thanksgiving from this dataset. You could also comment that approximately 81% of those surveyed eat Turkey as their main dish at Thanksgiving by calculating this by 859 that chose Turkey / 1,058 total surveyed.


thanksgiving%>%
  count(main_prep)

#The categories are below; Baked, Fried, Roasted, I don't know, other and NA.

#The data highlights that there were a combination of methods to prepare Thanksgiving main dishes however Baked & Roasted primarily contributed to 481 & 378 respectively or 45% & 36% with total of 81% of the main dishes. 
  
```

### 6. Create 3 different data viz showing insights for main dish served and the method. Provide your own legend and use themes.

### Write 2-3 sentences with your explanation of each insight. (4 marks)

```{r}

thanksgiving%>%
  filter(!is.na(main_dish))%>%
  ggplot(aes(main_dish, main_prep, colour = main_dish)) +
  geom_jitter() +
  labs(x = "Main Dish Types", y = "Preparation Type", title ="Viewing Thanksgiving Main Dish and Preparation Types",
subtitle = "Clearly Turkey is our main either Roasted or Baked...",
caption = "81% is Turkey & either Baked or Roasted...") +
  theme_classic() +
  coord_flip()
 
#Turkey is a Thanksgiving favourite. This data visualisation highlights to the viewers that clearly the most popular Thanksgiving meal is Turkey and this is split almost 50/50 by roasted or baked. You can see this as the x asis highlights the number of people and on the y axis the chosen main meals which colour is added to see the meal prep of those meals. The colour is added to easily see the main dishes individually in this visualisation. I have also used the filter function at the beginning to remove NA resposes and have a more clearer view of the data viz.

```

```{r}


thanksgiving%>%
  count(us_region)

#After running a count (above) on us_region we can see that the surveyed Thanksgiving respondents are from; South Atlantic 214 people, Middle Atlantic 159 people, East North Central 150 people & Pacific with 146 people as for the most part. The combination of these US regions equates to over 63% of the surveyed respondents. 

thanksgiving%>%
  filter(!is.na(main_dish))%>%
  ggplot(aes(main_dish, us_region, colour = us_region)) +
  labs(x = "Main Dish Types", y = "US Region", title ="Viewing Thanksgiving Main Dish by US Regions",
subtitle = "South Atlantic, Middle Atlantic, East North Central & Pacific have the most Turkey...") +
  theme_classic() +
  geom_jitter()

#You can then see this in the visualisation below where the main Thanksgiving dish is served highlighted by the us_region. Turkey as we know is the favourite coming from Middle Atlantic, South Atlantic, Pacific and East North Central. 
```

```{r}
thanksgiving%>%
  count(gender)

#I ran a count on gender and according to the data there was 481 males and 544 females with a 45% and 51% respectively. The below data visualisation has on the x asis main dish preparation types and the y axis the gender. We cannot see any major differences from the above numbers when exploring main dish preparation types. 

thanksgiving%>%
  filter(!is.na(gender))%>%
  ggplot(aes(main_prep, gender, colour = gender)) +
  labs(x = "Main Dish Preparation Types", y = "Gender", title ="Viewing Thanksgiving Main Dish Prep by Gender",
subtitle = "Almost a 50/50 split with Male & Female....") +
  theme_classic() +
  geom_jitter()

```

### 7. How many use cranberry sauce? How many use gravy? 2marks

```{r}
thanksgiving%>%
  count(cranberry)

thanksgiving%>%
  count(gravy)

#803 people surveyed used cranberry sauce and 892 people surveyed used gravy.

```

### 8 - 9 What is the distribution of those who celebrate across income ranges. Create a data viz. Write 2-3 sentences with your explanation of each insight. (4 marks)

```{r}
#First I had to change celebrate from a character into a factor in order to use it in any data visualisation 
thanksgiving<-thanksgiving%>%
  mutate(celebrate=as_factor(celebrate))

#I skimmed the data to confirm this change
skim(thanksgiving)

#I then filtered only those who celebrated Thanksgiving into my new dataset called celebrate.
celebrate<-thanksgiving%>%
  filter(celebrate=="Yes")

#I wanted to see a quick count of how many in each range. I realised that it was not ordered.
celebrate%>%
  count(family_income)

#I used fct_reorder and parse_number to organise the income ranges neatly.
celebrate<-celebrate%>%
  mutate(family_income=fct_reorder(family_income, parse_number(family_income)))

#This code below will highlight the distribution of family income of those who celebrated Thanksgiving.
celebrate%>%
  filter(!is.na(family_income))%>%
  ggplot(aes(family_income, colour = family_income)) +
  labs(x = "Family Income Ranges", y = "How Many People", title ="Distribution on Family Income across those who celebrated Thanksgiving",
subtitle = "...") +
  geom_bar() + 
  coord_flip()

#This data viz highlights that a large portion of those who celebrated Thanksgiving were on a family income of over $25,000 and below $125,000.
```

### 10. Use the following code to create a new data set 2 mark
### Write 2-3 sentences with your explanation of what it does. (4 marks)

```{r}


new_dataset<-thanksgiving%>%
select(id, starts_with("side"),
         starts_with("pie"),
         starts_with("dessert")) %>%
  select(-side15, -pie13, -dessert12) %>%
  gather(type, value, -id) %>%
  filter(!is.na(value),
         !value %in% c("None", "Other (please specify)")) %>%
  mutate(type = str_remove(type, "\\d+"))

#I believe what has happened here is firstly select. The new dataset called new_dataset will select from our original dataset called thanksgiving only these observations that match the pattern; side, pie & desert.
#It then pipes the dataset and runs select again but this time removing these three variables side15, pie13 & desert12.
#It then pipes again and gathers columns into key-value pairs; type, value and id.
#Then it pipes this and uses filter to remove from value any "None" and "Other" observations

#Overall what has happened here is that the original dataset was too difficult to use any side, pie or desert observations due to too many invalid observations. They either had nothing completed or had "None" or had "Other" which was too difficult to view. Now the new dataset can be used to compare within the three variables side, pie & desert without any errors.

```

### 11 - 12 Intall package `widyr` and use `pairwise_cor()` function https://www.rdocumentation.org/packages/widyr/versions/0.1.3/topics/pairwise_cor 

### Write 2-3 sentences with your explanation of what it does. (2 marks)

Use this code for the new dataset

`
pairwise_cor(value, id, sort = TRUE)

`
Write 1 sentence with your explanation of what insights it shows. (2 marks)

```{r}

library(widyr)

new_dataset%>%
  pairwise_cor(value, id, sort = TRUE)

#This shows the correlation between the two items and we can see that the first two rows have the highest explained variance between them as Cookies and Brownies. 
```


### 13. Use `lm()` or randomForest() function to build a model that predict a family income based on data in the dataset. 8 marks

Compare 3 models using different set of input variables. Use different number of variables.

Explain your choice of variables (3 sentences) 

Write 2 sentences explaining which model is best.

```{r}
#How to make numeric
skim(thanksgiving)

thanksgiving<-thanksgiving%>%
  mutate(us_region=as_factor(us_region),
         age=as_factor(age),
         gender=as_factor(gender),
         community_type=as_factor(community_type),
         family_income=as_factor(family_income))

result_1<-lm(family_income~us_region, data = thanksgiving)

summary(result_1)
tidy(result_1)

```


